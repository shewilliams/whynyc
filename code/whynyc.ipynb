{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9681366",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e139693",
   "metadata": {},
   "source": [
    "As a resource for social data, Twitter’s platform has been used to measure the quality of life through sentiment analysis. This capstone project explores another methodological technique of using specific keyword terms to determine dominant topics, word patterns, and sentiment leanings in a geographical area. Focusing on New York City and Los Angeles for comparative analysis, the keyword term “why” will be used to build a Python analysis around topic modeling and sentiment analysis. With this approach, the analysis reveals social and cultural differences, the overall sentiment of tweets, and areas of interest to tweeters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ed7a97",
   "metadata": {},
   "source": [
    "### Contents\n",
    "1. Install Libraries\n",
    "2. Import Python Libraries\n",
    "3. Data Setup *(import, query, convert JSON to DataFrame, and clean Twitter data)*\n",
    "4. Categorizing Sentiment on Tweets\n",
    "5. Exploratory Analysis\n",
    "6. Topic Modeling\n",
    "7. Topic Bubble Map\n",
    "8. Accuracy of Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cadda79",
   "metadata": {},
   "source": [
    "# Install libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e8f192",
   "metadata": {},
   "source": [
    "If you find that you are missing any libraries after importing them in the next step, please use this section to install them. For the nltk.download() functions, you are able to download them AFTER importing the NLTK library."
   ]
  },
  {
   "cell_type": "raw",
   "id": "d08218a8",
   "metadata": {},
   "source": [
    "pip install searchtweets-v2"
   ]
  },
  {
   "cell_type": "raw",
   "id": "46d0d858",
   "metadata": {},
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "raw",
   "id": "99463b92",
   "metadata": {},
   "source": [
    "#to import stop words from the English language\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0ed6b114",
   "metadata": {},
   "source": [
    "#to import word_tokenizer()\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5d267816",
   "metadata": {},
   "source": [
    "#to import SentimentIntensityAnalyzer()\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0cba8d23",
   "metadata": {},
   "source": [
    "pip install textblob"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ad9ca7c7",
   "metadata": {},
   "source": [
    "pip install contractions"
   ]
  },
  {
   "cell_type": "raw",
   "id": "859ab9a3",
   "metadata": {},
   "source": [
    "pip install wordcloud"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bf1a5667",
   "metadata": {},
   "source": [
    "pip install sklearn"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7d32e768",
   "metadata": {},
   "source": [
    "pip install plotly"
   ]
  },
  {
   "cell_type": "raw",
   "id": "81dde34b",
   "metadata": {},
   "source": [
    "pip install geopandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6acc04",
   "metadata": {},
   "source": [
    "# Python Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c97d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import searchtweets as twitter\n",
    "\n",
    "import pandas as pd\n",
    "from pandas.io.json import json_normalize\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "import contractions\n",
    "import string\n",
    "import textwrap\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from PIL import Image\n",
    "import plotly.graph_objs as go\n",
    "import plotly.express as px\n",
    "import geopandas as gpd\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "import warnings\n",
    "from warnings import simplefilter\n",
    "warnings.filterwarnings('ignore')\n",
    "simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53351d3d",
   "metadata": {},
   "source": [
    "# Data Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a146d46",
   "metadata": {},
   "source": [
    "## Import Twitter Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c10b254",
   "metadata": {},
   "source": [
    "The file *(twitter_keys.yaml)* in this code needs to be edited with your own tokens. Twitter does not allow the sharing of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ac3996",
   "metadata": {},
   "outputs": [],
   "source": [
    "#file contains API and Bearer tokens\n",
    "search_args = twitter.load_credentials('twitter_keys.yaml',\n",
    "                                     yaml_key='search_tweets_v2',\n",
    "                                     env_overwrite=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e40251",
   "metadata": {},
   "source": [
    "## Query Twitter Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7b2e52",
   "metadata": {},
   "source": [
    "To set up search terms for query. Twitter offers [documention](https://developer.twitter.com/en/docs/twitter-api/tweets/search/integrate/build-a-query) on building a query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef29d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up parameters for query search\n",
    "query_whynyc = twitter.gen_request_parameters(\n",
    "    query = 'why (place:01a9a39529b27f36 OR place:011add077f4d2da3 OR place:00c39537733fa112 OR place:002e24c6736f069d OR place:00c55f041e27dc51) -is:retweet -is:nullcast lang:en',\n",
    "    results_per_call = 500,\n",
    "    start_time = '2021-01-01', \n",
    "    end_time = '2022-01-01',\n",
    "    tweet_fields = 'id,created_at,text,geo',\n",
    "    granularity=''\n",
    ")\n",
    "\n",
    "#140k to get all of 2021\n",
    "whynyc_tweets = twitter.collect_results(\n",
    "    query_whynyc,\n",
    "    max_tweets=140000, \n",
    "    result_stream_args=search_args\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd3ca3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_whyla = twitter.gen_request_parameters(\n",
    "    query = 'why (place:3b77caf94bfc81fe) -is:retweet -is:nullcast lang:en',\n",
    "    results_per_call = 500,\n",
    "    start_time = '2021-01-01', \n",
    "    end_time = '2022-01-01',\n",
    "    tweet_fields = 'id,created_at,text,geo',\n",
    "    granularity=''\n",
    ")\n",
    "\n",
    "#90k\n",
    "whyla_tweets = twitter.collect_results(\n",
    "    query_whyla, \n",
    "    max_tweets=90000, \n",
    "    result_stream_args=search_args \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19a26da",
   "metadata": {},
   "source": [
    "## Convert JSON to Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f0cc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "whynyc_df = pd.json_normalize(whynyc_tweets, record_path=['data'])\n",
    "whyla_df = pd.json_normalize(whyla_tweets, record_path=['data'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608561e8",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927cf1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_links(tweet):\n",
    "    #function  extracts the links\n",
    "    return re.findall('(http\\S+|bit.ly/\\S+)', tweet)\n",
    "\n",
    "#def find_retweeted(tweet):\n",
    "   # function finds and extracts retweeted twitter handles\n",
    "    #return re.findall('(?<=RT\\s)(@[A-Za-z0-9]+[A-Za-z0-9-_]+)', tweet)\n",
    "\n",
    "def find_mentioned(tweet):\n",
    "    #function finds and extracts the twitter handles of people mentioned\n",
    "    return re.findall('(?<!RT\\s)(@[A-Za-z0-9]+[A-Za-z0-9-_]+)', tweet)  \n",
    "\n",
    "def find_hashtags(tweet):\n",
    "    #This function will extract hashtags\n",
    "    return re.findall('(#[A-Za-z0-9]+[A-Za-z0-9-_]+)', tweet)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae1e8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make new columns for links, retweeted usernames, mentioned usernames and hashtags\n",
    "whynyc_df['links'] = whynyc_df.text.apply(find_links)\n",
    "#whynyc_df['retweeted'] = whynyc_df.text.apply(find_retweeted)\n",
    "whynyc_df['mentioned'] = whynyc_df.text.apply(find_mentioned)\n",
    "whynyc_df['hashtags'] = whynyc_df.text.apply(find_hashtags)\n",
    "\n",
    "whyla_df['links'] = whyla_df.text.apply(find_links)\n",
    "#whyla_df['retweeted'] = whyla_df.text.apply(find_retweeted)\n",
    "whyla_df['mentioned'] = whyla_df.text.apply(find_mentioned)\n",
    "whyla_df['hashtags'] = whyla_df.text.apply(find_hashtags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46840efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to clean up the ['text'] column\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english') \n",
    "lemmatizer = WordNetLemmatizer() #groups together similar words as a single term\n",
    "punctuation = string.punctuation #'!'$%&\\'()*+,-./:;<=>?[\\\\]^_`{|}~•@'\n",
    "symbol = '—…«»“”‘’' #for symbols not captured in punctuation\n",
    "\n",
    "def clean_df(tweet):\n",
    "    #remove parts of a tweet\n",
    "    tweet = re.sub(r'http\\S+', '', tweet) #removes links\n",
    "    tweet = re.sub(r'bit.ly/\\S+', '', tweet) #removes bitly links\n",
    "    #tweet = re.sub('(RT\\s@[A-Za-z0-9]+[A-Za-z0-9-_]+)', '', tweet) #removes retweeted usernames\n",
    "    tweet = re.sub('(@[A-Za-z0-9]+[A-Za-z0-9-_]+)', '', tweet) #removes mentioned usernames\n",
    "    #removes hashtags, for this analysis they are kept in\n",
    "    #tweet = re.sub('(#[A-Za-z0-9]+[A-Za-z0-9-_]+)', '', tweet) \n",
    "    \n",
    "    #removing these that showup after data cleaning processing\n",
    "    tweet = re.sub('&amp;', '&', tweet)\n",
    "    tweet = re.sub('\\n', '', tweet)\n",
    "\n",
    "    #lower-case characters\n",
    "    tweet = tweet.lower()\n",
    "    \n",
    "    #remove contractions\n",
    "    tweet = contractions.fix(tweet)\n",
    "    \n",
    "    #remove numbers\n",
    "    tweet = re.sub('([0-9]+)', '', tweet)\n",
    "    \n",
    "    #remove punctuation\n",
    "    tweet = re.sub('['+ string.punctuation +']+', ' ', tweet)\n",
    "    \n",
    "    #remove symbols not captured in punctuation\n",
    "    tweet = re.sub('['+ symbol +']+', ' ', tweet)\n",
    "    \n",
    "    #remove whitespace\n",
    "    tweet = re.sub(r'^\\s+|\\s+$', '', tweet)\n",
    "    tweet = re.sub(r'\\s+', ' ', tweet)\n",
    "    \n",
    "    #tokenize words and remove stopwords\n",
    "    tweet_token_list = [word for word in tweet.split(' ')#]\n",
    "                            if word not in stopwords] # remove stopwords\n",
    "\n",
    "    #apply word lemmatization\n",
    "    tweet_token_list = [lemmatizer.lemmatize(word) if '#' not in word else word\n",
    "                        for word in tweet_token_list]\n",
    "    \n",
    "    tweet = ' '.join(tweet_token_list)\n",
    "    return tweet\n",
    "\n",
    "#create a new column for the cleaned text column.\n",
    "whynyc_df['corpora'] = whynyc_df.text.apply(clean_df)\n",
    "whyla_df['corpora'] = whyla_df.text.apply(clean_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e930a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pull list of columns\n",
    "list(whynyc_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ef0e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reorder columns in dataframe\n",
    "whynyc = whynyc_df[['id', 'created_at', 'text', 'corpora', 'geo.place_id']]\n",
    "whyla = whyla_df[['id', 'created_at', 'text', 'corpora', 'geo.place_id']]\n",
    "\n",
    "#created for a one-time analysis\n",
    "#adding this to whynyc and whyla dataframes may cause kernel to crash due to size\n",
    "whynyc_pot = whynyc_df[['mentioned', 'hashtags', 'links']]\n",
    "whyla_pot = whyla_df[['mentioned', 'hashtags', 'links']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95b01c5",
   "metadata": {},
   "source": [
    "## Categorizing Sentiment on Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e219880",
   "metadata": {},
   "source": [
    "Uses the NLTK and TextBlob libraries to calculate the polarity/sentiment (NLTK & TextBlob) and subjectivity (TextBlob only) scores of tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4054ade2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sid = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71884760",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_sentiment(why_df):\n",
    "    #pulling polarity scores from NLTK library\n",
    "    sa_nltk_list = []\n",
    "    for i in why_df['text']:\n",
    "        sa_nltk_list.append((sid.polarity_scores(str(i)))['compound'])\n",
    "    #why_df['score'] = pd.Series(sa_nltk_list, dtype='float64')\n",
    "    \n",
    "    #pulling subjectivity and polarity scores from TextBlob\n",
    "    def subjectivity(text): \n",
    "        return TextBlob(text).sentiment.subjectivity\n",
    "    why_df['subjectivity'] = why_df['text'].apply(subjectivity)\n",
    "    \n",
    "    #Create a function to get the polarity\n",
    "    sa_tb_list = []\n",
    "    def polarity(text): \n",
    "        return TextBlob(text).sentiment.polarity\n",
    "    sa_tb_list = why_df['text'].apply(polarity)\n",
    "    \n",
    "    #average of NLTK and TextBlob's polarity scores via Numpy\n",
    "    avg = []\n",
    "    avg = np.mean(np.array([sa_nltk_list, sa_tb_list]), axis=0)\n",
    "    why_df['score'] = pd.DataFrame(avg)\n",
    "    \n",
    "    #Categorizing sentiment scores\n",
    "    def sentiment_category(sentiment):\n",
    "        label = ''\n",
    "        if(sentiment>0):\n",
    "            label = 'positive'\n",
    "        elif(sentiment == 0):\n",
    "            label = 'neutral'\n",
    "        else:\n",
    "            label = 'negative'    \n",
    "        return(label)\n",
    "    why_df['sentiment'] = why_df['score'].apply(sentiment_category)\n",
    "    \n",
    "    return why_df\n",
    "\n",
    "whynyc = add_sentiment(whynyc)\n",
    "whyla = add_sentiment(whyla)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9474cff9",
   "metadata": {},
   "source": [
    "# Exploratory Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f63629",
   "metadata": {},
   "source": [
    "Due to links, mentions, and hahtags accounting for less than 1/3 of the total tweets queried, this portion will only provide a basic idea of how much relevance the parts of a tweet (below) play a role. In a future project, a network analysis will come into play for this part.discourse.\n",
    "1. Links (either to an image or a website)\n",
    "2. Mentioned\n",
    "3. Hashtags\n",
    "\n",
    "\n",
    "\n",
    "<table>\n",
    "  <thead>\n",
    "    <tr>\n",
    "      <th> </th>\n",
    "      <th>New York City</th>\n",
    "      <th>Los Angeles</th>\n",
    "      <th>% of tweets (NYC/LA)</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td>Links accounts for </td>\n",
    "      <td>41,566 tweets</td>\n",
    "      <td>24,497 tweets</td>\n",
    "      <td>(30%/27%)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Mentioned tweeters account for</td>\n",
    "      <td>46,067 tweets</td>\n",
    "      <td>28,535 tweets</td>\n",
    "      <td>(33%/32%)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Hashtags account for</td>\n",
    "      <td>9,180 tweets</td>\n",
    "      <td>5,537 tweets</td>\n",
    "      <td>(7%/6%)</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec241b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(whynyc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed832bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(whyla)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4272d044",
   "metadata": {},
   "outputs": [],
   "source": [
    "whynyc_pot['links'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfadb375",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "whynyc_pot['mentioned'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7688e69c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "whynyc_pot['hashtags'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a175ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "whyla_pot['links'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0b5d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "whyla_pot['mentioned'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efed30f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "whyla_pot['hashtags'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc3d026",
   "metadata": {},
   "source": [
    "## Text Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5413ce",
   "metadata": {},
   "source": [
    "This portion converts the 'created_at' column into datatime format with to_datatime() function in pandas. Length of tweets and a basic time series analysis is performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087b8fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_why_df(why_df):\n",
    "    #convert created_at column to datetime\n",
    "    why_df['datetime'] = pd.to_datetime(why_df['created_at'], errors='coerce')\n",
    "\n",
    "    #create a day column\n",
    "    why_df['day'] = why_df['datetime'].dt.date\n",
    "\n",
    "    #create a month column\n",
    "    why_df['month'] = why_df['datetime'].dt.month\n",
    "    \n",
    "    #break up text column into length\n",
    "    why_df['length']=why_df['text'].apply(lambda x:len(x.split()))\n",
    "    return why_df\n",
    "\n",
    "whynyc = format_why_df(whynyc)\n",
    "whyla = format_why_df(whyla)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc45fe6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "whynyc['length'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff72f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "whyla['length'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7a6e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set colors for tweets categorized as positive, neutral, or negative\n",
    "sentiment_colors = {\n",
    "    'positive': '#2A9D8F',\n",
    "    'neutral': '#847979',\n",
    "    'negative': '#F4A259'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94327c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creates a graph of length of tweets by sentiment in a histogram\n",
    "px.histogram(\n",
    "    whynyc, \n",
    "    x='length', \n",
    "    color='sentiment', \n",
    "    color_discrete_map = sentiment_colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd56b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(\n",
    "    whyla, \n",
    "    x='length', \n",
    "    color='sentiment', \n",
    "    color_discrete_map = sentiment_colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62abfa88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert dataframe to lists\n",
    "whynyc_list = whynyc['corpora'].values.tolist()\n",
    "whynyc_list = ' '.join(whynyc_list).lower()\n",
    "\n",
    "whyla_list = whyla['corpora'].values.tolist() \n",
    "whyla_list = ' '.join(whyla_list).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34f62cf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#create a frequency distribution and graph it\n",
    "fdist_whynyc = FreqDist(word_tokenize(whynyc_list))\n",
    "plt.figure(figsize=(10, 4))\n",
    "fdist_whynyc.plot(30, cumulative=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae11e246",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fdist_whyla = FreqDist(word_tokenize(whyla_list))\n",
    "plt.figure(figsize=(10, 4))\n",
    "fdist_whyla.plot(30,cumulative=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62bddc2d",
   "metadata": {},
   "source": [
    "## Text Analysis - Word Clouds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea7967e",
   "metadata": {},
   "source": [
    "Creates a word cloud based on the text ('corpora') data for NYC and LA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a795f3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def create_wordcloud(file_name, list_name):\n",
    "    #pull the image file\n",
    "    mask = np.array(Image.open(file_name))\n",
    "    #function converts RGB values from 0 (black) to white (255)\n",
    "    def transform_zeros(val):\n",
    "        if val == 0:\n",
    "            return 255\n",
    "        else:\n",
    "            return val\n",
    "    #map and create a mask for image\n",
    "    maskable_image = np.ndarray((mask.shape[0],mask.shape[1]), np.int32)\n",
    "    for i in range(len(mask)):\n",
    "        maskable_image[i] = list(map(transform_zeros, mask[i]))\n",
    "\n",
    "    #create word cloud\n",
    "    wordcloud = WordCloud(\n",
    "        width = 3000, \n",
    "        height = 2000, \n",
    "        #random_state=1, \n",
    "        background_color='white', \n",
    "        colormap='twilight_r', \n",
    "        contour_width = 1,\n",
    "        contour_color = '#111954',\n",
    "        collocations=True, \n",
    "        stopwords = STOPWORDS, \n",
    "        mask=maskable_image).generate(list_name)\n",
    "\n",
    "    def plot_cloud(wordcloud):\n",
    "        # Set figure size\n",
    "        plt.figure(figsize=(15, 7))\n",
    "        # Display image\n",
    "        plt.imshow(wordcloud) \n",
    "        # No axis details\n",
    "        plt.axis('off');\n",
    "    return plot_cloud(wordcloud)\n",
    "\n",
    "create_wordcloud('new-york-city.png', whynyc_list)\n",
    "create_wordcloud('los-angeles.png', whyla_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d423ad8b",
   "metadata": {},
   "source": [
    "## N-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5cc923",
   "metadata": {},
   "source": [
    "N-grams are continuous sequences of a neighbouring sequences of terms in a document. This section will look at n-grams up to 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb48f507",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngrams(text, ngram_from=2, ngram_to=2, n=None, max_features=20000):\n",
    "    \n",
    "    vectorizer = TfidfVectorizer(ngram_range = (ngram_from, ngram_to), \n",
    "                          max_features = max_features, \n",
    "                          stop_words='english').fit(text)\n",
    "    bag_of_words = vectorizer.transform(text)\n",
    "    sum_words = bag_of_words.sum(axis = 0) \n",
    "    words_freq = [(word, sum_words[0, i]) for word, i in vectorizer.vocabulary_.items()]\n",
    "    words_freq = sorted(words_freq, key = lambda x: x[1], reverse = True)\n",
    "   \n",
    "    return words_freq[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7464f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngrams_table(why_df):\n",
    "    unigrams = pd.DataFrame(get_ngrams(why_df['corpora'], ngram_from=1, ngram_to=1, n=15))\n",
    "    bigrams = pd.DataFrame(get_ngrams(why_df['corpora'], ngram_from=2, ngram_to=2, n=15))\n",
    "    trigrams = pd.DataFrame(get_ngrams(why_df['corpora'], ngram_from=3, ngram_to=3, n=15))\n",
    "    quadgrams = pd.DataFrame(get_ngrams(why_df['corpora'], ngram_from=4, ngram_to=4, n=15))\n",
    "    \n",
    "    ngrams = pd.concat([unigrams, bigrams, trigrams, quadgrams], axis = 1)\n",
    "    ngrams.columns = ['unigrams', 'frequency', 'bigrams', 'frequency', 'trigrams', 'frequency', 'quadgrams', 'frequency']\n",
    "    ngrams\n",
    "    return ngrams\n",
    "\n",
    "ngrams_nyc = ngrams_table(whynyc)\n",
    "ngrams_la = ngrams_table(whyla)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d30f2e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ngrams_nyc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0507551",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ngrams_la"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203e7b71",
   "metadata": {},
   "source": [
    "## Sentiment EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a520b63c",
   "metadata": {},
   "source": [
    "This section is an exploratory data analysis of the sentiment on tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe0c50e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "whynyc['score'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfab2d83",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "whynyc['sentiment'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20232e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "whynyc['sentiment'].value_counts().plot(ax=ax, kind='bar', xlabel='numbers', ylabel='frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82281530",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "whyla['sentiment'].value_counts().plot(ax=ax, kind='bar', xlabel='numbers', ylabel='frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb25494",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pol_and_sub_of_tweets(title, why_df):\n",
    "    # plot the polarity and subjectivity\n",
    "    fig = px.scatter(why_df, \n",
    "                     x='score', \n",
    "                     y='subjectivity', \n",
    "                     color = 'sentiment',\n",
    "                     color_discrete_map = sentiment_colors,\n",
    "                     size='subjectivity',\n",
    "                     hover_name=why_df.text.apply(lambda txt: '<br>'.join(textwrap.wrap(txt, width=35))))\n",
    "\n",
    "    #add a vertical line at x=0 for Netural Reviews\n",
    "    fig.update_layout(title=title,\n",
    "                      shapes=[dict(type= 'line',\n",
    "                                   yref= 'paper', y0= 0, y1= 1, \n",
    "                                   xref= 'x', x0= 0, x1= 0)])\n",
    "    return fig.show()\n",
    "\n",
    "pol_and_sub_of_tweets('Subjectivity and Polarity Scores of Tweets in NYC', whynyc)\n",
    "pol_and_sub_of_tweets('Subjectivity and Polarity Scores of Tweets in LA', whyla)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8664122c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_count(why_df):\n",
    "    sent_count = why_df.groupby('day').sentiment.value_counts()\n",
    "    sent_count = sent_count.to_frame(name='count')\n",
    "    sent_count.reset_index(inplace=True)\n",
    "    return sent_count\n",
    "\n",
    "whynyc_sent = sentiment_count(whynyc)\n",
    "whyla_sent = sentiment_count(whyla)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cba332b-3ece-4cd1-ae3d-f96839a04948",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def sentiment_plotly(why_df):\n",
    "    fig = go.Figure()\n",
    "    for c in why_df['sentiment'].unique()[:3]:\n",
    "        dfp = why_df[why_df['sentiment']==c].pivot(\n",
    "            index='day', \n",
    "            columns='sentiment', \n",
    "            values='count')\n",
    "        \n",
    "        fig.add_traces(\n",
    "            go.Scatter(\n",
    "                x=dfp.index, \n",
    "                y=dfp[c], \n",
    "                mode='lines', \n",
    "                name = c))\n",
    "    return fig.show()\n",
    "\n",
    "sentiment_plotly(whynyc_sent)\n",
    "sentiment_plotly(whyla_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7280d2d1",
   "metadata": {},
   "source": [
    "The next few word clouds explore the spikes in sentiment on particular days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9378570f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc_neg_wc_j6 = whynyc[(whynyc['datetime']>='2021-01-05') & (whynyc['datetime']<='2021-01-08') & (whynyc['sentiment']=='negative')]\n",
    "wordcloud = WordCloud(max_font_size=50, max_words=500, background_color='white').generate(str(nyc_neg_wc_j6['corpora']))\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5944cae3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "la_neg_wc_j6 = whyla[(whyla['datetime']>='2021-01-05') & (whyla['datetime']<='2021-01-08') & (whyla['sentiment']=='negative')]\n",
    "wordcloud = WordCloud(max_font_size=50, max_words=500, background_color='white').generate(str(la_neg_wc_j6['corpora']))\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea90d095",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "positive_wc_m29 = whynyc[(whynyc['datetime']>='2021-03-29') & (whynyc['datetime']<='2021-03-30') & (whynyc['sentiment']=='positive')]\n",
    "wordcloud = WordCloud(max_font_size=50, max_words=500, background_color='white').generate(str(positive_wc_m29['corpora']))\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1270ed51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweets_timeline_plotly(why_df):\n",
    "    fig = px.scatter(why_df,\n",
    "                 x='datetime',\n",
    "                 y='length',\n",
    "                 #range_y=[50, 120],\n",
    "                 color_discrete_map = sentiment_colors,\n",
    "                 color='sentiment',\n",
    "                 size='length',\n",
    "                 hover_name=why_df.text.apply(lambda txt: '<br>'.join(textwrap.wrap(txt, width=35)))\n",
    "                )\n",
    "    return fig.show()\n",
    "\n",
    "tweets_timeline_plotly(whynyc)\n",
    "tweets_timeline_plotly(whyla)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08933ca5",
   "metadata": {},
   "source": [
    "# Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f6b82f",
   "metadata": {},
   "source": [
    "Topic modeling is a text mining tool to reveal semantic structures of a body of text to reveal abstract topics that occur. It is a probabalistic model that will document which specific topic has certain words appearing more frequently than others. From the scikit-learn library, the Latent Dirichlet Allocation and TfidfVectorizer are used to build this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5e9168",
   "metadata": {},
   "outputs": [],
   "source": [
    "def why_lda_model(why_df):\n",
    "    vectorizer = TfidfVectorizer(max_df=0.9, min_df=25, token_pattern='\\w+|\\$[\\d\\.]+|\\S+', ngram_range=(1,3))#max_df=0.9, min_df=25,)\n",
    "\n",
    "    # apply transformation\n",
    "    tf = vectorizer.fit_transform(why_df['corpora']).toarray()\n",
    "\n",
    "    # tf_feature_names tells us what word each column in the matric represents\n",
    "    tf_feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "    number_of_topics = 30\n",
    "\n",
    "    model = LatentDirichletAllocation(n_components=number_of_topics, random_state=0)\n",
    "    model.fit(tf)\n",
    "\n",
    "    #creates a table of the topics and weight of the document\n",
    "    def display_topics(model, feature_names, no_top_words):\n",
    "        topic_dict = {}\n",
    "        for topic_idx, topic in enumerate(model.components_):\n",
    "            topic_dict['Topic %d words' % (topic_idx)]= ['{}'.format(feature_names[i])\n",
    "                            for i in topic.argsort()[:-no_top_words - 1:-1]]\n",
    "            topic_dict['Topic %d weights' % (topic_idx)]= ['{:.1f}'.format(topic[i])\n",
    "                            for i in topic.argsort()[:-no_top_words - 1:-1]]\n",
    "        return pd.DataFrame(topic_dict)\n",
    "\n",
    "    no_top_words = 15\n",
    "    return display_topics(model, tf_feature_names, no_top_words).T\n",
    "    \n",
    "whynyc_lda = why_lda_model(whynyc)   \n",
    "whyla_lda = why_lda_model(whyla) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290635a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_topics(x, why_df):\n",
    "    #creates a dataframe of the extracted topics from the previous cell box\n",
    "    topic_df = pd.DataFrame()\n",
    "\n",
    "    #extracts the first column of topics for every other row\n",
    "    topic_df['topic'] = x.iloc[::2, :1].reset_index(drop=True)\n",
    "    #extracts the first column of weights for every other row\n",
    "    topic_df['weight'] = x.iloc[1::2, :1].reset_index(drop=True)\n",
    "    #combines the other columns of topics sans the calculated weight\n",
    "    topic_df['subtopics'] = x.iloc[::2, 1:].apply(lambda x: ', '.join(x[x.notnull()]), axis = 1).reset_index(drop=True)\n",
    "\n",
    "    #calculates the overall average of sentiment (polarity) based on the topic extracted in line 6\n",
    "    values = []\n",
    "    for word in topic_df['topic']:     \n",
    "        temp_list = why_df.loc[why_df['text'].str.contains(word, case=False)].reset_index()\n",
    "        mean_of_topic = temp_list['score'].mean()\n",
    "        values.append(mean_of_topic)\n",
    "        values = [0 if x != x else x for x in values]\n",
    "        topic_df['sentiment'] = pd.DataFrame(values)\n",
    "        topic_df = topic_df.sort_values(by=['weight'], ascending=False, ignore_index=True)\n",
    "        \n",
    "        #categorizw the sentiment based on score\n",
    "        def sentiment_category(sentiment):\n",
    "            label = ''\n",
    "            if(sentiment>0):\n",
    "                label = 'positive'\n",
    "            elif(sentiment == 0):\n",
    "                label = 'neutral'\n",
    "            else:\n",
    "                label = 'negative'    \n",
    "            return(label)\n",
    "    \n",
    "    topic_df['category'] = topic_df['sentiment'].apply(sentiment_category)   \n",
    "    return topic_df\n",
    "      \n",
    "whynyc_topics = collect_topics(whynyc_lda, whynyc)\n",
    "whyla_topics = collect_topics(whyla_lda, whyla)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3660ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to display all the items in the subtopics column\n",
    "pd.set_option('display.max_colwidth', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62a4055",
   "metadata": {},
   "outputs": [],
   "source": [
    "whynyc_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa41e9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "whyla_topics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e343881",
   "metadata": {},
   "source": [
    "# Topic Bubble Map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577f5379",
   "metadata": {},
   "source": [
    "This data visualization creates a topic bubble map based on the results in the *Topic Modeling* section. Geopandas is used to generate a set of random latitude and longitude coordinates within the geographical boundaries of the area of interest. Plotly is used to graph everything together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b210a34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_map(file_name, why_df, export_name, title):\n",
    "    #read GeoJSON file\n",
    "    gdf_polys = gpd.read_file(file_name)\n",
    "\n",
    "    # find the bounds of your geodataframe\n",
    "    x_min, y_min, x_max, y_max = gdf_polys.total_bounds\n",
    "    # set sample size\n",
    "    n = 100\n",
    "    # generate random data within the bounds\n",
    "    x = np.random.uniform(x_min, x_max, n)\n",
    "    y = np.random.uniform(y_min, y_max, n)\n",
    "\n",
    "    # convert them to a points GeoSeries\n",
    "    gdf_points = gpd.GeoSeries(gpd.points_from_xy(x, y))\n",
    "    # only keep those points within polygons\n",
    "    gdf_points = gdf_points[gdf_points.within(gdf_polys.unary_union)]\n",
    "    \n",
    "    #reset thet index of both dataframes and add the lat and lon columns\n",
    "    gdf_points = gdf_points.reset_index(drop=True)\n",
    "    why_df = why_df.reset_index(drop=True)\n",
    "    why_df['lon'] = gdf_points.geometry.apply(lambda p: p.x)\n",
    "    why_df['lat'] = gdf_points.geometry.apply(lambda p: p.y)\n",
    "    \n",
    "    #create the geographical scatter plot\n",
    "    fig = px.scatter_mapbox(\n",
    "        why_df,\n",
    "        lat=why_df['lat'],\n",
    "        lon=why_df['lon'],\n",
    "        zoom=8.5,\n",
    "        hover_name=why_df['topic'],\n",
    "        text = why_df.subtopics.apply(lambda txt: '<br>'.join(textwrap.wrap(txt, width=35))),\n",
    "        width = 600,\n",
    "        height = 700,\n",
    "    )\n",
    "    \n",
    "    #set color of scatter points\n",
    "    def SetColor(x):\n",
    "        if(x < 0):\n",
    "            return '#F4A259'\n",
    "        elif(x == 0):\n",
    "            return '#847979'\n",
    "        elif(x > 0):\n",
    "            return '#2A9D8F'\n",
    "\n",
    "    #set scatter points\n",
    "    fig.update_traces(\n",
    "        mode='markers+text',\n",
    "        \n",
    "        marker=dict(\n",
    "            color= list(map(SetColor, why_df['sentiment'])),\n",
    "            size=why_df['weight'].astype(float)/7,\n",
    "        ),\n",
    "        \n",
    "        showlegend=True,\n",
    "        hovertemplate= '<b>Topic: '+ why_df.topic + '</b><br><br>' \n",
    "            + 'The associated words with this topic are: <br>%{text}<br><br>'\n",
    "            + 'The overall sentiment is '\n",
    "            + why_df.category + '.',\n",
    "    )\n",
    "\n",
    "    #update and customize map\n",
    "    fig.update_layout(\n",
    "        mapbox = {\n",
    "            'style': 'carto-positron',\n",
    "            'layers': [\n",
    "                {\n",
    "                'source': file_name,\n",
    "                'type': 'fill',\n",
    "                    'below': 'traces',\n",
    "                    'color': '#111954',\n",
    "                    'opacity': 0.4,\n",
    "                    'line': {'width': 5}\n",
    "                } \n",
    "            ]},\n",
    "        hoverlabel=dict(\n",
    "            bgcolor='white',\n",
    "            bordercolor='white',\n",
    "            font=dict(color='black'),\n",
    "            font_size=12, \n",
    "            font_family='Helvetica', \n",
    "            align='left'\n",
    "        ),\n",
    "        legend_title_text = '<b>'+ title + '</b>',\n",
    "        legend=dict(\n",
    "            orientation='h',\n",
    "            yanchor='top',\n",
    "            y=0.99,\n",
    "            xanchor='left',\n",
    "            x=0.01\n",
    "        ),\n",
    "\n",
    "    )\n",
    "    \n",
    "    #write Plotly graph to an HTML file\n",
    "    fig.write_html(export_name, full_html=False, include_plotlyjs='cdn')\n",
    "\n",
    "    fig.show()\n",
    "    return topic_map\n",
    "\n",
    "nyc_map = topic_map('nyc.geojson', whynyc_topics, 'nyc.html', 'Why, New York City?')\n",
    "la_map = topic_map('la.geojson', whyla_topics, 'la.html', 'Why, Los Angeles?')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3b08bd",
   "metadata": {},
   "source": [
    "# Accuracy of Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f29e97",
   "metadata": {},
   "source": [
    "This purpose of this section is to determine whether or not the methodology behind the sentiment analysis is accurate using Logisitic Regression and Multinomial Naive Bayes from the sklearn library.\n",
    "\n",
    "For a recap, to get both the subjectivity and polarity scores on tweets, the NLTK and TextBlob libraries were used. NLTK does not have a number associate with their subjectivity library; therefore, the mean was calculated for the NLTK and TextBlob polarity scores. Lastly, this was converted into 'positive' (scores above 0), 'negative' (scores less than 0) and 'neutral' (scores equal to 0) values. A manual look-through of the dataset, there are some tweets that appear to be miscategorized between the sentiment categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b70a77",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def model_accuracy(df_name, why_df):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        why_df['text'], \n",
    "        why_df['sentiment'], \n",
    "        test_size=0.2, \n",
    "        random_state=24)\n",
    "    \n",
    "    vectorizer = TfidfVectorizer(ngram_range=(1,3), stop_words='english')\n",
    "    X_train = vectorizer.fit_transform(X_train)\n",
    "    X_test = vectorizer.transform(X_test)\n",
    "    \n",
    "    #Logistic Regression\n",
    "    lr = LogisticRegression()\n",
    "    lr.fit(X_train, y_train)\n",
    "    \n",
    "    predictions = lr.predict(X_test)\n",
    "    confusion_matrix(predictions,y_test)\n",
    "    print(classification_report(predictions,y_test))\n",
    "    \n",
    "    accuracy = metrics.accuracy_score(predictions, y_test)\n",
    "    print(str('For the ' + df_name + ' dataset, the accuracy for Logistic Regression with TfidfVectorizer is {:04.2f}'.format(accuracy*100))+'%')\n",
    "    \n",
    "    #Naive Bayes (Multinomial)\n",
    "    mnb = MultinomialNB()\n",
    "    mnb.fit(X_train,y_train)\n",
    "    predictions_mnb = mnb.predict(X_test)\n",
    "    confusion_matrix(predictions_mnb,y_test)\n",
    "    print(classification_report(predictions_mnb,y_test))\n",
    "    accuracy_mnb = metrics.accuracy_score(predictions_mnb, y_test)\n",
    "    \n",
    "    return print(str('For the ' + df_name + ' dataset, the accuracy for Multinomial Naive Bayes with TfidfVectorizer is {:04.2f}'.format(accuracy_mnb*100))+'%')\n",
    "\n",
    "model_accuracy('whynyc', whynyc)\n",
    "model_accuracy('whyla', whyla)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
